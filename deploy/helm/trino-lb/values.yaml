image:
  repository: docker.stackable.tech/stackable/trino-lb
  pullPolicy: IfNotPresent

replicas: 3

resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: "1"
    memory: 256Mi

redis-cluster:
  password: redis
  persistence:
    size: 4Gi
  cluster:
# In case the master crashes, one of his slaves will be promoted to master. The slots stored by the crashed master will be unavailable until the slave finish the promotion. If a master and all his slaves crash, the cluster will be down until one of them is up again. To avoid downtime, it is possible to configure the number of RedisÂ® nodes with cluster.nodes and the number of replicas that will be assigned to each master with cluster.replicas. For example:
# *cluster.nodes=9 ( 3 master plus 2 replicas for each master)
# * cluster.replicas=2
#
# So we configure the cluster to have 3 masters (minimum amount required) and, each master, will have 2 replicas.
    nodes: 9
    replicas: 2
  podDisruptionBudget:
    maxUnavailable: 1
  redis:
    resources:
      requests:
        cpu: 200m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  service:
    type: LoadBalancer

kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      # By default, Prometheus discovers PodMonitors and ServiceMonitors within its namespace, that are labeled with the same release tag as the prometheus-operator release. Sometimes, you may need to discover custom PodMonitors/ServiceMonitors, for example used to scrape data from third-party applications. An easy way of doing this, without compromising the default PodMonitors/ServiceMonitors discovery, is allowing Prometheus to discover all PodMonitors/ServiceMonitors within its namespace, without applying label filtering. To do so, you can set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues and prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues to false.
      podMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      ruleSelector: {}
      serviceMonitorSelector: {}
    service:
      type: LoadBalancer # NodePort
      nodePort: null
  grafana:
    grafana.ini:
      analytics:
        check_for_updates: false
    adminPassword: "adminadmin"
    persistence:
      enabled: true
      size: 5Gi
    service:
      type: LoadBalancer # NodePort
      nodePort: null
    dashboardProviders:
      trino-lb-dashboardprovider.yaml:
        apiVersion: 1
        providers:
        - name: 'trino-lb'
          orgId: 1
          folder: 'trino-lb'
          type: file
          disableDeletion: false
          allowUiUpdates: true # TODO set to false. Is enabled so Dashboards can be build easier. Note: If a provisioned dashboard is saved from the UI and then later updated from the source, the dashboard stored in the database will always be overwritten
          options:
            path: /var/lib/grafana/dashboards/stackable # Must be /var/lib/grafana/dashboards/<provider_name>
    dashboardsConfigMaps:
      stackable: trino-lb-grafana-dashboards
  kube-state-metrics:
    metricLabelsAllowlist:
      - pods=[*]
      - statefulsets=[*]
      - services=[*]
      - nodes=[*]
      - namespaces=[*]
      - namespaces=[*]
      - configmaps=[*]
      - persistentvolumes=[*]
      - persistentvolumeclaims=[*]
