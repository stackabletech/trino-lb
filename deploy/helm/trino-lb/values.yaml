image:
  repository: oci.stackable.tech/stackable/trino-lb
  tag: dev
  pullPolicy: Always

replicas: 3

resources:
  requests:
    cpu: 250m
    memory: 256Mi
  limits:
    cpu: "1"
    memory: 256Mi

redis-cluster:
  password: redis
  persistence:
    size: 4Gi
  cluster:
# In case the master crashes, one of his slaves will be promoted to master.
# The slots stored by the crashed master will be unavailable until the slave finish the promotion.
# If a master and all his slaves crash, the cluster will be down until one of them is up again.
# To avoid downtime, it is possible to configure the number of RedisÂ® nodes with cluster.nodes and the number of replicas that will be assigned to each master with cluster.replicas. For example:
# *cluster.nodes=6 ( 3 master plus 1 replica for each master)
# * cluster.replicas=1
#
# So we configure the cluster to have 3 masters (minimum amount required) and, each master, will have 1 replicas.
# You can obviously upper the numbers
    nodes: 6
    replicas: 1
  podDisruptionBudget:
    maxUnavailable: 1
  redis:
    resources:
      requests:
        cpu: 200m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 128Mi
  service:
    type: ClusterIP

kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      # By default, Prometheus discovers PodMonitors and ServiceMonitors within its namespace, that are labeled with the same release tag as the prometheus-operator release. Sometimes, you may need to discover custom PodMonitors/ServiceMonitors, for example used to scrape data from third-party applications. An easy way of doing this, without compromising the default PodMonitors/ServiceMonitors discovery, is allowing Prometheus to discover all PodMonitors/ServiceMonitors within its namespace, without applying label filtering. To do so, you can set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues and prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues to false.
      podMonitorSelectorNilUsesHelmValues: false
      serviceMonitorSelectorNilUsesHelmValues: false
      ruleSelector: {}
      serviceMonitorSelector: {}
    service:
      type: LoadBalancer # NodePort
      nodePort: null
  grafana:
    grafana.ini:
      analytics:
        check_for_updates: false
    adminPassword: "adminadmin"
    persistence:
      enabled: true
      size: 5Gi
    service:
      type: LoadBalancer # NodePort
      nodePort: null
    dashboardProviders:
      trino-lb-dashboardprovider.yaml:
        apiVersion: 1
        providers:
        - name: 'trino-lb'
          orgId: 1
          folder: 'trino-lb'
          type: file
          disableDeletion: false
          allowUiUpdates: true # TODO set to false. Is enabled so Dashboards can be build easier. Note: If a provisioned dashboard is saved from the UI and then later updated from the source, the dashboard stored in the database will always be overwritten
          options:
            path: /var/lib/grafana/dashboards/stackable # Must be /var/lib/grafana/dashboards/<provider_name>
    dashboardsConfigMaps:
      stackable: trino-lb-grafana-dashboards
  kube-state-metrics:
    metricLabelsAllowlist:
      - pods=[*]
      - statefulsets=[*]
      - services=[*]
      - nodes=[*]
      - namespaces=[*]
      - namespaces=[*]
      - configmaps=[*]
      - persistentvolumes=[*]
      - persistentvolumeclaims=[*]
